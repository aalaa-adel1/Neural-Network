{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7889639,"sourceType":"datasetVersion","datasetId":4631923}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-21T01:28:44.195560Z","iopub.execute_input":"2024-03-21T01:28:44.195994Z","iopub.status.idle":"2024-03-21T01:28:44.201290Z","shell.execute_reply.started":"2024-03-21T01:28:44.195959Z","shell.execute_reply":"2024-03-21T01:28:44.200220Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"2.15.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Modified VGG16**","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_dir = \"/kaggle/input/neuralnetweorkdata/dataset/train\"\ntest_dir = \"/kaggle/input/neuralnetweorkdata/dataset/test\"\n\n# Set the target image size\nimg_width, img_height = 224, 224\n\n# Set the batch size\nbatch_size = 32\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2  # 20% of the data will be used for validation\n)\n\n# Create the training data generator\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'  # Specify that this is the training subset\n)\n\n# Create the validation data generator\nvalidation_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'  # Specify that this is the validation subset\n)\n\nmodel = Sequential()\n\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(img_width, img_height, 3)))\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D(64,(2, 2)))\n\nmodel.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(4096, activation='relu'))  # 3000 = No of neurons\nmodel.add(Dropout(0.1))  # regularization\nmodel.add(Dense(4096, activation='relu'))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(train_generator.num_classes, activation='softmax'))\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model\nepochs =20\nmodel.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n\n# Calculate accuracy on the validation set\nval_loss, val_accuracy = model.evaluate(validation_generator)\nprint(\"Validation accuracy:\", val_accuracy)\n\ntest_files = [f for f in os.listdir(test_dir) if os.path.isfile(os.path.join(test_dir, f))]\n\ntest_images = []\nfor filename in test_files:\n    img_path = os.path.join(test_dir, filename)\n    img = load_img(img_path, target_size=(img_width, img_height))\n    img_array = img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n    test_images.append(img_array)\n\n# Convert the list of images to a numpy array\ntest_images = np.array(test_images)\n\n# Predict the classes for the test images\npredictions = model.predict(test_images)\n\n# Convert predictions to class labels\npredicted_labels = np.argmax(predictions, axis=1) + 1\n\n# Print the predicted labels\nprint(\"Predicted labels:\", predicted_labels)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T01:28:44.947167Z","iopub.execute_input":"2024-03-21T01:28:44.948088Z","iopub.status.idle":"2024-03-21T02:29:35.857639Z","shell.execute_reply.started":"2024-03-21T01:28:44.948054Z","shell.execute_reply":"2024-03-21T02:29:35.856484Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Found 7920 images belonging to 5 classes.\nFound 1980 images belonging to 5 classes.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1710984597.971636     255 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1710984597.992626     255 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728ms/step - accuracy: 0.2059 - loss: 1.6005","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1710984780.320142     254 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 935ms/step - accuracy: 0.2060 - loss: 1.6004 - val_accuracy: 0.3561 - val_loss: 1.4011\nEpoch 2/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 693ms/step - accuracy: 0.4087 - loss: 1.2720 - val_accuracy: 0.4773 - val_loss: 1.1892\nEpoch 3/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 691ms/step - accuracy: 0.5314 - loss: 1.1058 - val_accuracy: 0.4702 - val_loss: 1.2383\nEpoch 4/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 691ms/step - accuracy: 0.5590 - loss: 1.0300 - val_accuracy: 0.5409 - val_loss: 1.1191\nEpoch 5/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 690ms/step - accuracy: 0.5990 - loss: 0.9741 - val_accuracy: 0.5737 - val_loss: 1.1025\nEpoch 6/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 688ms/step - accuracy: 0.6156 - loss: 0.9293 - val_accuracy: 0.5697 - val_loss: 1.0652\nEpoch 7/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 686ms/step - accuracy: 0.6285 - loss: 0.9303 - val_accuracy: 0.5379 - val_loss: 1.1287\nEpoch 8/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 695ms/step - accuracy: 0.6466 - loss: 0.8856 - val_accuracy: 0.5702 - val_loss: 1.0651\nEpoch 9/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 686ms/step - accuracy: 0.6669 - loss: 0.8477 - val_accuracy: 0.6000 - val_loss: 1.0024\nEpoch 10/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 685ms/step - accuracy: 0.6613 - loss: 0.8488 - val_accuracy: 0.6005 - val_loss: 0.9883\nEpoch 11/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 686ms/step - accuracy: 0.6835 - loss: 0.8000 - val_accuracy: 0.6000 - val_loss: 1.0337\nEpoch 12/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 690ms/step - accuracy: 0.7024 - loss: 0.7812 - val_accuracy: 0.5803 - val_loss: 1.0730\nEpoch 13/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 687ms/step - accuracy: 0.7078 - loss: 0.7657 - val_accuracy: 0.6106 - val_loss: 1.0148\nEpoch 14/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 690ms/step - accuracy: 0.7129 - loss: 0.7509 - val_accuracy: 0.5995 - val_loss: 1.0619\nEpoch 15/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 688ms/step - accuracy: 0.7217 - loss: 0.7357 - val_accuracy: 0.5975 - val_loss: 0.9978\nEpoch 16/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 687ms/step - accuracy: 0.7364 - loss: 0.7172 - val_accuracy: 0.6419 - val_loss: 0.9303\nEpoch 17/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 687ms/step - accuracy: 0.7411 - loss: 0.6913 - val_accuracy: 0.6061 - val_loss: 0.9962\nEpoch 18/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 686ms/step - accuracy: 0.7486 - loss: 0.6691 - val_accuracy: 0.6106 - val_loss: 0.9942\nEpoch 19/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 687ms/step - accuracy: 0.7411 - loss: 0.6830 - val_accuracy: 0.6172 - val_loss: 1.0453\nEpoch 20/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 686ms/step - accuracy: 0.7635 - loss: 0.6291 - val_accuracy: 0.6348 - val_loss: 0.9575\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 449ms/step - accuracy: 0.6282 - loss: 0.9640\nValidation accuracy: 0.6348484754562378\n\u001b[1m2/4\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 193ms/step","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1710988169.817867     256 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step  \nPredicted labels: [2 4 5 2 2 2 5 1 4 1 3 3 1 5 2 3 4 3 3 1 4 5 5 4 2 1 1 1 2 4 5 1 1 3 4 3 1\n 1 4 4 1 5 2 3 2 1 2 5 4 4 4 4 5 2 4 2 5 3 5 5 4 2 5 5 2 3 2 4 5 5 3 3 5 1\n 4 1 2 2 5 2 5 1 2 5 5 2 4 4 1 3 1 3 4 4 1 2 1 2 3 4]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# CNN MODEL","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_dir = \"/kaggle/input/neuralnetweorkdata/dataset/train\"\ntest_dir = \"/kaggle/input/neuralnetweorkdata/dataset/test\"\n\n# Set the target image size\nimg_width, img_height = 299, 299\n\n# Set the batch size\nbatch_size = 32\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2  # 20% of the data will be used for validation\n)\n\n# Create the training data generator\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'  # Specify that this is the training subset\n)\n\n# Create the validation data generator\nvalidation_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'  # Specify that this is the validation subset\n)\n\nmodel = Sequential()\n\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(img_width, img_height, 3)))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(3000, activation='relu'))  # 3000 = No of neurons\nmodel.add(Dropout(0.1))  # regularization\nmodel.add(Dense(3000, activation='relu'))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(train_generator.num_classes, activation='softmax'))\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model\nepochs =20\n\nmodel.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n\n# Calculate accuracy on the validation set\nval_loss, val_accuracy = model.evaluate(validation_generator)\nprint(\"Validation accuracy:\", val_accuracy)\n\ntest_files = [f for f in os.listdir(test_dir) if os.path.isfile(os.path.join(test_dir, f))]\n\ntest_images = []\nfor filename in test_files:\n    img_path = os.path.join(test_dir, filename)\n    img = load_img(img_path, target_size=(img_width, img_height))\n    img_array = img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n    test_images.append(img_array)\n\n# Convert the list of images to a numpy array\ntest_images = np.array(test_images)\n\n# Predict the classes for the test images\npredictions = model.predict(test_images)\n\n# Convert predictions to class labels\npredicted_labels = np.argmax(predictions, axis=1) + 1\n\n# Print the predicted labels\nprint(\"Predicted labels:\", predicted_labels)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T02:29:35.860059Z","iopub.execute_input":"2024-03-21T02:29:35.860854Z","iopub.status.idle":"2024-03-21T03:50:16.884322Z","shell.execute_reply.started":"2024-03-21T02:29:35.860816Z","shell.execute_reply":"2024-03-21T03:50:16.883232Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Found 7920 images belonging to 5 classes.\nFound 1980 images belonging to 5 classes.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1710988243.010954     255 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m 57/248\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:17\u001b[0m 1s/step - accuracy: 0.2519 - loss: 1.5583   ","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1710988318.596575     254 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865ms/step - accuracy: 0.3867 - loss: 1.3570","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1710988459.142468     257 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 1s/step - accuracy: 0.3871 - loss: 1.3564 - val_accuracy: 0.5212 - val_loss: 1.1344\nEpoch 2/20\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1710988522.397184     256 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 926ms/step - accuracy: 0.5834 - loss: 1.0175 - val_accuracy: 0.5056 - val_loss: 1.2141\nEpoch 3/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 919ms/step - accuracy: 0.6397 - loss: 0.9031 - val_accuracy: 0.5702 - val_loss: 1.0399\nEpoch 4/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 919ms/step - accuracy: 0.6753 - loss: 0.8254 - val_accuracy: 0.5955 - val_loss: 1.0000\nEpoch 5/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 918ms/step - accuracy: 0.7055 - loss: 0.7559 - val_accuracy: 0.6460 - val_loss: 0.8979\nEpoch 6/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 917ms/step - accuracy: 0.7342 - loss: 0.6845 - val_accuracy: 0.6606 - val_loss: 0.9157\nEpoch 7/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 925ms/step - accuracy: 0.7534 - loss: 0.6526 - val_accuracy: 0.6778 - val_loss: 0.8668\nEpoch 8/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 919ms/step - accuracy: 0.7664 - loss: 0.6007 - val_accuracy: 0.6692 - val_loss: 0.8871\nEpoch 9/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 925ms/step - accuracy: 0.7998 - loss: 0.5329 - val_accuracy: 0.6404 - val_loss: 0.9621\nEpoch 10/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 926ms/step - accuracy: 0.8192 - loss: 0.4891 - val_accuracy: 0.6773 - val_loss: 0.9003\nEpoch 11/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 929ms/step - accuracy: 0.8356 - loss: 0.4403 - val_accuracy: 0.6571 - val_loss: 0.9828\nEpoch 12/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 928ms/step - accuracy: 0.8656 - loss: 0.3695 - val_accuracy: 0.6955 - val_loss: 0.9372\nEpoch 13/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 921ms/step - accuracy: 0.8926 - loss: 0.3067 - val_accuracy: 0.6914 - val_loss: 0.9856\nEpoch 14/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 925ms/step - accuracy: 0.9067 - loss: 0.2669 - val_accuracy: 0.6823 - val_loss: 1.0731\nEpoch 15/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 915ms/step - accuracy: 0.9244 - loss: 0.2145 - val_accuracy: 0.6793 - val_loss: 1.2097\nEpoch 16/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 922ms/step - accuracy: 0.9358 - loss: 0.1827 - val_accuracy: 0.6727 - val_loss: 1.2201\nEpoch 17/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 920ms/step - accuracy: 0.9523 - loss: 0.1402 - val_accuracy: 0.6833 - val_loss: 1.2490\nEpoch 18/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 918ms/step - accuracy: 0.9436 - loss: 0.1639 - val_accuracy: 0.6753 - val_loss: 1.3200\nEpoch 19/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 915ms/step - accuracy: 0.9636 - loss: 0.1083 - val_accuracy: 0.6990 - val_loss: 1.1978\nEpoch 20/20\n\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 923ms/step - accuracy: 0.9718 - loss: 0.0914 - val_accuracy: 0.6889 - val_loss: 1.3663\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 723ms/step - accuracy: 0.7039 - loss: 1.2849\nValidation accuracy: 0.6954545378684998\n\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1710993011.714334     257 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step \nPredicted labels: [3 4 5 2 2 2 1 1 4 1 3 3 1 5 2 3 4 4 3 5 4 5 5 2 2 1 1 1 5 2 5 1 1 3 4 3 5\n 1 4 4 5 5 2 3 2 3 2 5 4 4 2 3 5 4 4 4 5 3 1 5 3 3 5 1 2 3 2 3 5 1 3 3 5 1\n 2 1 2 2 5 2 5 5 2 5 5 2 4 4 3 3 3 3 2 4 1 2 1 2 3 3]\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1710993016.861045     254 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install einops\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T03:50:16.885723Z","iopub.execute_input":"2024-03-21T03:50:16.886009Z","iopub.status.idle":"2024-03-21T03:50:33.537214Z","shell.execute_reply.started":"2024-03-21T03:50:16.885982Z","shell.execute_reply":"2024-03-21T03:50:33.536010Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m570.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.7.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Transformers","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.models import resnet18\nfrom torch.nn import functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import random_split\nfrom torchvision import transforms\nfrom einops import rearrange\nimport math \nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport sys\n\n# Set the target image size\nimg_width, img_height = 299, 299\n\n# Set the batch size\nbatch_size = 50\n\n# Set the paths for the train and test directories\ntrain_dir = \"/kaggle/input/neuralnetweorkdata/dataset/train\"\ntest_dir = \"/kaggle/input/neuralnetweorkdata/dataset/test\"\n# Set up image data generators with data augmentation for the training set\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop((img_height, img_width)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n])\n\n# Create the training data loader with the defined transformations\ntrain_dataset = ImageFolder(train_dir, transform=train_transform)\n\n# Split the dataset into training and validation sets\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\n# Create data loaders for training and validation sets\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n        super(PatchEmbedding, self).__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.in_channels = in_channels\n        self.embed_dim = embed_dim\n\n    def forward(self, x):\n        x = rearrange(x, 'b c h w -> b (h w c)')\n        #print(\"Shape after rearrangement:\", x.shape)\n        \n        # Initialize the linear layer based on the shape of x\n        self.projection = nn.Linear(x.shape[1], self.embed_dim)\n        x = self.projection(x)\n        return x\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, embed_dim, max_seq_len=512):\n        super(PositionalEncoding, self).__init__()\n        self.embed_dim = embed_dim\n        self.register_buffer('pe', self._get_positional_encoding(max_seq_len, embed_dim))\n\n    def _get_positional_encoding(self, max_seq_len, embed_dim):\n        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))\n        pe = torch.zeros(max_seq_len, embed_dim)\n        pe[:, 0::2] = torch.sin(position * div_term) #even positions\n        pe[:, 1::2] = torch.cos(position * div_term)  #odd positions\n        pe = pe.unsqueeze(0)\n        return pe\n\n    def forward(self, x):\n        # Check if x is 3D (batch_size, seq_len, embed_dim)\n        if x.dim() == 3:\n            return x + self.pe[:, :x.size(1)].detach()\n        else:\n            raise ValueError(f\"Unsupported input shape: {x.shape}\")\n\n\n\n\n\n# Transformer Encoder block\n\nclass TransformerEncoderBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, mlp_dim):\n        super(TransformerEncoderBlock, self).__init__()\n        self.embed_dim = embed_dim\n        self.ln1 = nn.LayerNorm(embed_dim)\n        self.attention = nn.MultiheadAttention(embed_dim, num_heads) #num_head==> more than head (hyperparam)\n        self.ln2 = nn.LayerNorm(embed_dim)\n        self.mlp = MLPBlock(embed_dim, mlp_dim)\n\n    def forward(self, x):\n        # Self-attention\n        residual = x\n        x = self.ln1(x)\n        x = self.attention(x, x, x)[0]\n        x = x + residual\n\n        # MLP\n        residual = x\n        x = self.ln2(x)\n        x = self.mlp(x)\n        x = x + residual\n\n        return x\n    \n    \n# MLP block\nclass MLPBlock(nn.Sequential):  # training here of features\n    def __init__(self, embed_dim, mlp_dim):\n        super(MLPBlock, self).__init__(\n            nn.Linear(embed_dim, mlp_dim), #layer\n            nn.GELU(), #act fn \n            nn.Dropout(0.1), #no overfitting\n            nn.Linear(mlp_dim, embed_dim), #layer\n            nn.Dropout(0.1)\n        )\n        \n        \nclass VisionTransformer(nn.Module):\n    def __init__(self, img_size, patch_size, in_channels, num_classes, embed_dim, num_heads, mlp_dim, num_layers):\n        super(VisionTransformer, self).__init__()\n        \n        self.patch_embedding = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n        self.positional_encoding = PositionalEncoding(embed_dim)\n        self.transformer_blocks = nn.ModuleList([\n            TransformerEncoderBlock(embed_dim, num_heads, mlp_dim)\n            for _ in range(num_layers)\n        ])\n\n        self.ln = nn.LayerNorm(embed_dim)\n        self.fc = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        x = self.patch_embedding(x)\n        x = x.view(x.size(0), -1, self.patch_embedding.embed_dim)  # Reshape to (batch_size, seq_len, embed_dim)\n        x = self.positional_encoding(x)\n        \n        for block in self.transformer_blocks:\n            x = block(x)\n\n        x = self.ln(x.mean(dim=1))  # Global average pooling\n        x = self.fc(x)\n\n        return x\n\n# Initialize the model\nmodel = VisionTransformer(\n    img_size=(img_width, img_height),\n    patch_size=16,\n    in_channels=3,\n    num_classes=5,\n    embed_dim=256, #size of vector that out from embbeding #before = 256\n    num_heads=8, #before = 8 \n    mlp_dim=512,  #before=512\n    num_layers=6 #before=6\n)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters()) #upadte weights\n\nimport sys\n\nepochs =1\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    correct_train = 0\n    total_train = 0\n\n    # Print the length of the training generator\n    print(f'Training generator length: {len(train_loader)}')\n\n    # Use the data generated by train_generator\n    for i, (inputs, labels) in enumerate(train_loader):\n        # Convert inputs and labels to PyTorch tensors\n        inputs, labels = torch.tensor(inputs), torch.tensor(labels)\n\n        # Ensure the inputs are in the correct format (NCHW)\n        inputs = inputs.permute(0, 3, 1, 2).contiguous()\n\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        # Calculate training accuracy\n        _, predicted = torch.max(outputs, 1)\n        total_train += labels.size(0)\n        correct_train += (predicted == labels.argmax(dim=0)).sum().item()\n\n        # Print loss for each iteration\n        print(f'Epoch {epoch + 1}/{epochs}, Iteration {i + 1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n\n        # Break the loop if you want to iterate only up to len(train_generator) batches\n        if i + 1 == len(train_loader):\n            break\n\n    # Calculate training accuracy\n    train_accuracy = (correct_train / total_train) * 100\n        # Validation phase\n    model.eval()\n    correct_val = 0\n    total_val = 0\n\n    with torch.no_grad():\n        for inputs_val, labels_val in val_loader:\n            # Convert inputs and labels to PyTorch tensors\n            inputs_val, labels_val = torch.tensor(inputs_val), torch.tensor(labels_val)\n\n            # Ensure the inputs are in the correct format (NCHW)\n            inputs_val = inputs_val.permute(0, 3, 1, 2).contiguous()\n\n            # Make predictions\n            outputs_val = model(inputs_val)\n            _, predicted_val = torch.max(outputs_val, 1)\n            total_val += labels_val.size(0)\n            correct_val += (predicted_val == labels_val.argmax(dim=0)).sum().item()\n\n    # Calculate validation accuracy\n    val_accuracy = (correct_val / total_val) * 100\n\n    # Print epoch information with validation accuracy\n    print(f'Epoch {epoch + 1}/{epochs}, '\n          f'Training Loss: {running_loss / len(train_loader):.4f}, '\n          f'Training Accuracy: {train_accuracy:.2f}%, '\n          f'Validation Accuracy: {val_accuracy:.2f}%')\n    sys.stdout.flush()  # Flush the output but\n# Prepare the test data\ntest_files = [f for f in os.listdir(test_dir) if os.path.isfile(os.path.join(test_dir, f))]\n\ntest_images = []\nfor filename in test_files:\n    img_path = os.path.join(test_dir, filename)\n    img = load_img(img_path, target_size=(img_width, img_height))\n    img_array = img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n    test_images.append(img_array)\n\n# Convert the list of images to a numpy array\ntest_images = np.array(test_images)\n\n# Convert numpy array to PyTorch tensor\ntest_images = torch.tensor(np.transpose(test_images, (0, 3, 1, 2)), dtype=torch.float32)\n\n\n# Make predictions on the test set\nwith torch.no_grad():\n    model.eval()\n\n    predictions = model(test_images)\n\n# Convert predictions to class labels\npredicted_labels = torch.argmax(predictions, axis=1).numpy() + 1\n\n# Print the predicted labels\nprint(\"Predicted labels:\", predicted_labels)\n\n# (Insert code for testing the model on the test dataset here)    ","metadata":{"execution":{"iopub.status.busy":"2024-03-21T03:50:33.540094Z","iopub.execute_input":"2024-03-21T03:50:33.540455Z","iopub.status.idle":"2024-03-21T03:54:55.768589Z","shell.execute_reply.started":"2024-03-21T03:50:33.540420Z","shell.execute_reply":"2024-03-21T03:54:55.767356Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Training generator length: 159\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_197/2705071380.py:189: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  inputs, labels = torch.tensor(inputs), torch.tensor(labels)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1, Iteration 1/159, Loss: 1.6976\nEpoch 1/1, Iteration 2/159, Loss: 3.7139\nEpoch 1/1, Iteration 3/159, Loss: 3.0375\nEpoch 1/1, Iteration 4/159, Loss: 2.8774\nEpoch 1/1, Iteration 5/159, Loss: 1.8612\nEpoch 1/1, Iteration 6/159, Loss: 1.8918\nEpoch 1/1, Iteration 7/159, Loss: 1.8015\nEpoch 1/1, Iteration 8/159, Loss: 1.5874\nEpoch 1/1, Iteration 9/159, Loss: 1.6943\nEpoch 1/1, Iteration 10/159, Loss: 1.6908\nEpoch 1/1, Iteration 11/159, Loss: 1.7590\nEpoch 1/1, Iteration 12/159, Loss: 1.7151\nEpoch 1/1, Iteration 13/159, Loss: 1.7087\nEpoch 1/1, Iteration 14/159, Loss: 1.6564\nEpoch 1/1, Iteration 15/159, Loss: 1.6628\nEpoch 1/1, Iteration 16/159, Loss: 1.6400\nEpoch 1/1, Iteration 17/159, Loss: 1.6800\nEpoch 1/1, Iteration 18/159, Loss: 1.6084\nEpoch 1/1, Iteration 19/159, Loss: 1.6886\nEpoch 1/1, Iteration 20/159, Loss: 1.6559\nEpoch 1/1, Iteration 21/159, Loss: 1.7105\nEpoch 1/1, Iteration 22/159, Loss: 1.6070\nEpoch 1/1, Iteration 23/159, Loss: 1.6408\nEpoch 1/1, Iteration 24/159, Loss: 1.6672\nEpoch 1/1, Iteration 25/159, Loss: 1.6547\nEpoch 1/1, Iteration 26/159, Loss: 1.6271\nEpoch 1/1, Iteration 27/159, Loss: 1.6177\nEpoch 1/1, Iteration 28/159, Loss: 1.6263\nEpoch 1/1, Iteration 29/159, Loss: 1.6456\nEpoch 1/1, Iteration 30/159, Loss: 1.6188\nEpoch 1/1, Iteration 31/159, Loss: 1.6302\nEpoch 1/1, Iteration 32/159, Loss: 1.6537\nEpoch 1/1, Iteration 33/159, Loss: 1.7156\nEpoch 1/1, Iteration 34/159, Loss: 1.6089\nEpoch 1/1, Iteration 35/159, Loss: 1.6581\nEpoch 1/1, Iteration 36/159, Loss: 1.6498\nEpoch 1/1, Iteration 37/159, Loss: 1.6326\nEpoch 1/1, Iteration 38/159, Loss: 1.6186\nEpoch 1/1, Iteration 39/159, Loss: 1.6167\nEpoch 1/1, Iteration 40/159, Loss: 1.6730\nEpoch 1/1, Iteration 41/159, Loss: 1.6248\nEpoch 1/1, Iteration 42/159, Loss: 1.6218\nEpoch 1/1, Iteration 43/159, Loss: 1.6326\nEpoch 1/1, Iteration 44/159, Loss: 1.6365\nEpoch 1/1, Iteration 45/159, Loss: 1.6065\nEpoch 1/1, Iteration 46/159, Loss: 1.6314\nEpoch 1/1, Iteration 47/159, Loss: 1.6570\nEpoch 1/1, Iteration 48/159, Loss: 1.6307\nEpoch 1/1, Iteration 49/159, Loss: 1.6268\nEpoch 1/1, Iteration 50/159, Loss: 1.6000\nEpoch 1/1, Iteration 51/159, Loss: 1.6813\nEpoch 1/1, Iteration 52/159, Loss: 1.6474\nEpoch 1/1, Iteration 53/159, Loss: 1.7331\nEpoch 1/1, Iteration 54/159, Loss: 1.7166\nEpoch 1/1, Iteration 55/159, Loss: 1.6527\nEpoch 1/1, Iteration 56/159, Loss: 1.6783\nEpoch 1/1, Iteration 57/159, Loss: 1.6631\nEpoch 1/1, Iteration 58/159, Loss: 1.6219\nEpoch 1/1, Iteration 59/159, Loss: 1.5861\nEpoch 1/1, Iteration 60/159, Loss: 1.5650\nEpoch 1/1, Iteration 61/159, Loss: 1.6461\nEpoch 1/1, Iteration 62/159, Loss: 1.7033\nEpoch 1/1, Iteration 63/159, Loss: 1.5879\nEpoch 1/1, Iteration 64/159, Loss: 1.6517\nEpoch 1/1, Iteration 65/159, Loss: 1.6362\nEpoch 1/1, Iteration 66/159, Loss: 1.6337\nEpoch 1/1, Iteration 67/159, Loss: 1.6439\nEpoch 1/1, Iteration 68/159, Loss: 1.5908\nEpoch 1/1, Iteration 69/159, Loss: 1.5977\nEpoch 1/1, Iteration 70/159, Loss: 1.6129\nEpoch 1/1, Iteration 71/159, Loss: 1.6203\nEpoch 1/1, Iteration 72/159, Loss: 1.6846\nEpoch 1/1, Iteration 73/159, Loss: 1.6601\nEpoch 1/1, Iteration 74/159, Loss: 1.6031\nEpoch 1/1, Iteration 75/159, Loss: 1.6268\nEpoch 1/1, Iteration 76/159, Loss: 1.6477\nEpoch 1/1, Iteration 77/159, Loss: 1.6040\nEpoch 1/1, Iteration 78/159, Loss: 1.6297\nEpoch 1/1, Iteration 79/159, Loss: 1.5805\nEpoch 1/1, Iteration 80/159, Loss: 1.6479\nEpoch 1/1, Iteration 81/159, Loss: 1.6010\nEpoch 1/1, Iteration 82/159, Loss: 1.6213\nEpoch 1/1, Iteration 83/159, Loss: 1.6907\nEpoch 1/1, Iteration 84/159, Loss: 1.6874\nEpoch 1/1, Iteration 85/159, Loss: 1.6159\nEpoch 1/1, Iteration 86/159, Loss: 1.6365\nEpoch 1/1, Iteration 87/159, Loss: 1.6410\nEpoch 1/1, Iteration 88/159, Loss: 1.6643\nEpoch 1/1, Iteration 89/159, Loss: 1.6478\nEpoch 1/1, Iteration 90/159, Loss: 1.5985\nEpoch 1/1, Iteration 91/159, Loss: 1.6474\nEpoch 1/1, Iteration 92/159, Loss: 1.5994\nEpoch 1/1, Iteration 93/159, Loss: 1.6345\nEpoch 1/1, Iteration 94/159, Loss: 1.6420\nEpoch 1/1, Iteration 95/159, Loss: 1.6244\nEpoch 1/1, Iteration 96/159, Loss: 1.6005\nEpoch 1/1, Iteration 97/159, Loss: 1.6248\nEpoch 1/1, Iteration 98/159, Loss: 1.6155\nEpoch 1/1, Iteration 99/159, Loss: 1.6433\nEpoch 1/1, Iteration 100/159, Loss: 1.6281\nEpoch 1/1, Iteration 101/159, Loss: 1.6429\nEpoch 1/1, Iteration 102/159, Loss: 1.6726\nEpoch 1/1, Iteration 103/159, Loss: 1.6352\nEpoch 1/1, Iteration 104/159, Loss: 1.6125\nEpoch 1/1, Iteration 105/159, Loss: 1.6364\nEpoch 1/1, Iteration 106/159, Loss: 1.6106\nEpoch 1/1, Iteration 107/159, Loss: 1.5632\nEpoch 1/1, Iteration 108/159, Loss: 1.6580\nEpoch 1/1, Iteration 109/159, Loss: 1.6940\nEpoch 1/1, Iteration 110/159, Loss: 1.6690\nEpoch 1/1, Iteration 111/159, Loss: 1.6190\nEpoch 1/1, Iteration 112/159, Loss: 1.6352\nEpoch 1/1, Iteration 113/159, Loss: 1.6328\nEpoch 1/1, Iteration 114/159, Loss: 1.6168\nEpoch 1/1, Iteration 115/159, Loss: 1.6216\nEpoch 1/1, Iteration 116/159, Loss: 1.6486\nEpoch 1/1, Iteration 117/159, Loss: 1.6601\nEpoch 1/1, Iteration 118/159, Loss: 1.6118\nEpoch 1/1, Iteration 119/159, Loss: 1.6274\nEpoch 1/1, Iteration 120/159, Loss: 1.6438\nEpoch 1/1, Iteration 121/159, Loss: 1.6053\nEpoch 1/1, Iteration 122/159, Loss: 1.6144\nEpoch 1/1, Iteration 123/159, Loss: 1.6215\nEpoch 1/1, Iteration 124/159, Loss: 1.6222\nEpoch 1/1, Iteration 125/159, Loss: 1.6655\nEpoch 1/1, Iteration 126/159, Loss: 1.6525\nEpoch 1/1, Iteration 127/159, Loss: 1.6014\nEpoch 1/1, Iteration 128/159, Loss: 1.6288\nEpoch 1/1, Iteration 129/159, Loss: 1.5888\nEpoch 1/1, Iteration 130/159, Loss: 1.6156\nEpoch 1/1, Iteration 131/159, Loss: 1.6337\nEpoch 1/1, Iteration 132/159, Loss: 1.6153\nEpoch 1/1, Iteration 133/159, Loss: 1.6372\nEpoch 1/1, Iteration 134/159, Loss: 1.6105\nEpoch 1/1, Iteration 135/159, Loss: 1.5838\nEpoch 1/1, Iteration 136/159, Loss: 1.6549\nEpoch 1/1, Iteration 137/159, Loss: 1.7471\nEpoch 1/1, Iteration 138/159, Loss: 1.8014\nEpoch 1/1, Iteration 139/159, Loss: 1.7266\nEpoch 1/1, Iteration 140/159, Loss: 1.6556\nEpoch 1/1, Iteration 141/159, Loss: 1.5957\nEpoch 1/1, Iteration 142/159, Loss: 1.6505\nEpoch 1/1, Iteration 143/159, Loss: 1.6776\nEpoch 1/1, Iteration 144/159, Loss: 1.6426\nEpoch 1/1, Iteration 145/159, Loss: 1.5986\nEpoch 1/1, Iteration 146/159, Loss: 1.6739\nEpoch 1/1, Iteration 147/159, Loss: 1.6138\nEpoch 1/1, Iteration 148/159, Loss: 1.6619\nEpoch 1/1, Iteration 149/159, Loss: 1.5769\nEpoch 1/1, Iteration 150/159, Loss: 1.5812\nEpoch 1/1, Iteration 151/159, Loss: 1.6140\nEpoch 1/1, Iteration 152/159, Loss: 1.6477\nEpoch 1/1, Iteration 153/159, Loss: 1.6614\nEpoch 1/1, Iteration 154/159, Loss: 1.6402\nEpoch 1/1, Iteration 155/159, Loss: 1.6298\nEpoch 1/1, Iteration 156/159, Loss: 1.5927\nEpoch 1/1, Iteration 157/159, Loss: 1.6457\nEpoch 1/1, Iteration 158/159, Loss: 1.6233\nEpoch 1/1, Iteration 159/159, Loss: 1.5939\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_197/2705071380.py:223: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  inputs_val, labels_val = torch.tensor(inputs_val), torch.tensor(labels_val)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1, Training Loss: 1.6736, Training Accuracy: 10.35%, Validation Accuracy: 10.10%\nPredicted labels: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 5 5 5 5 4 5 5]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Saving Result for Submission on kaggle","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nresult_df = pd.DataFrame({\n    'image_id': [filename.split('.')[0] for filename in test_files],  # Extracting image names without extension\n    'label': predicted_labels\n})\n\n# Save the DataFrame to a CSV file\nresult_csv_path = \"/kaggle/working/comp5.csv\"\nresult_df.to_csv(result_csv_path, index=False)\n\n# Print the DataFrame\nprint(result_df)\n\n# Print a message indicating the CSV file path\nprint(f\"Predictions saved to: {result_csv_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-21T03:54:55.770017Z","iopub.execute_input":"2024-03-21T03:54:55.770665Z","iopub.status.idle":"2024-03-21T03:54:55.830599Z","shell.execute_reply.started":"2024-03-21T03:54:55.770633Z","shell.execute_reply":"2024-03-21T03:54:55.829709Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"   image_id  label\n0      1266      5\n1      1862      5\n2       311      5\n3      3362      5\n4      3652      5\n..      ...    ...\n95     3813      5\n96      639      5\n97     3449      4\n98     1656      5\n99     1910      5\n\n[100 rows x 2 columns]\nPredictions saved to: /kaggle/working/comp5.csv\n","output_type":"stream"}]}]}